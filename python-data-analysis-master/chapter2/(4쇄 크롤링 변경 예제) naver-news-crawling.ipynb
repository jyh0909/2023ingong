{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2023 [윤기태]\n",
    "\n",
    "https://github.com/yoonkt200/python-data-analysis\n",
    "\n",
    "[MIT License](https://github.com/yoonkt200/python-data-analysis/blob/master/LICENSE.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (가제) 파이썬 데이터 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <이것이 데이터 분석이다 with 파이썬> 4쇄 이후 변경된 크롤링 파트\n",
    "- 네이버 뉴스 헤드라인 데이터 크롤링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 바로가기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [<Step1. 크롤링> : 크롤링으로 웹 데이터 가져오기](#<Step1.-크롤링>-:-크롤링으로-웹-데이터-가져오기)\n",
    "    - [BeautifulSoup을 이용한 웹 크롤링]\n",
    "    - [네이버 뉴스 헤드라인 데이터 크롤링]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 맥 OS, 리눅스 OS의 경우 /{Local Path}/anaconda3/envs/{env name}/lib/python3.{xx}/site-packages/pytagcloud/fonts의 경로에 위와 동일한 방법을 적용해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <Step1. 크롤링> : 크롤링으로 웹 데이터 가져오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [BeautifulSoup을 이용한 웹 크롤링]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 아래 코드 실행을 위해, anaconda prompt 혹은 Terminal에서 아래와 같은 패키지들을 설치해 줍니다.\n",
    "    - (env_name) `pip install selenium`\n",
    "    - (env_name) `pip install beautifulsoup4`\n",
    "- 혹은 아래의 코드로 라이브러리를 설치합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.8.2-py3-none-any.whl (6.9 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /opt/homebrew/anaconda3/envs/pybook/lib/python3.9/site-packages (4.11.2)\n",
      "Collecting trio-websocket~=0.9\n",
      "  Using cached trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
      "Collecting trio~=0.17\n",
      "  Using cached trio-0.22.0-py3-none-any.whl (384 kB)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in /opt/homebrew/anaconda3/envs/pybook/lib/python3.9/site-packages (from selenium) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /opt/homebrew/anaconda3/envs/pybook/lib/python3.9/site-packages (from selenium) (2022.12.7)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/homebrew/anaconda3/envs/pybook/lib/python3.9/site-packages (from beautifulsoup4) (2.4)\n",
      "Requirement already satisfied: sniffio in /opt/homebrew/anaconda3/envs/pybook/lib/python3.9/site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: idna in /opt/homebrew/anaconda3/envs/pybook/lib/python3.9/site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/homebrew/anaconda3/envs/pybook/lib/python3.9/site-packages (from trio~=0.17->selenium) (22.2.0)\n",
      "Collecting exceptiongroup>=1.0.0rc9\n",
      "  Using cached exceptiongroup-1.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting sortedcontainers\n",
      "  Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Collecting async-generator>=1.9\n",
      "  Using cached async_generator-1.10-py3-none-any.whl (18 kB)\n",
      "Collecting outcome\n",
      "  Using cached outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting wsproto>=0.14\n",
      "  Using cached wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Collecting PySocks!=1.5.7,<2.0,>=1.5.6\n",
      "  Using cached PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Collecting h11<1,>=0.9.0\n",
      "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: sortedcontainers, PySocks, outcome, h11, exceptiongroup, async-generator, wsproto, trio, trio-websocket, selenium\n",
      "Successfully installed PySocks-1.7.1 async-generator-1.10 exceptiongroup-1.1.0 h11-0.14.0 outcome-1.2.0 selenium-4.8.2 sortedcontainers-2.4.0 trio-0.22.0 trio-websocket-0.9.2 wsproto-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "- 책에 기재된 나무위키 페이지 대신, 네이버 뉴스 홈의 특정 섹터를 크롤링하는 예제로 변경하였습니다.\n",
    "- 아래 이미지는 '정치' 섹터의 헤드라인 뉴스 크롤링을 하기 위한 페이지입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![앱 생성 페이지](img/new_crawl_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 페이지 리스트 가져오기 (헤드라인 뉴스들의 링크 가져오기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# brew 로 설치된 chromedriver의 path (Mac)\n",
    "mac_path = \"/usr/local/bin/chromedriver\"  # m1 이전 맥 OS\n",
    "# mac_path = \"/opt/homebrew/bin/chromedriver\"  # m1 이후 맥 OS\n",
    "\n",
    "# 윈도우용 크롬 웹드라이버 실행 경로 (Windows)\n",
    "window_path = \"chromedriver.exe\"\n",
    "\n",
    "# 크롤링할 사이트 주소를 입력합니다.\n",
    "source_url = \"https://news.naver.com/main/main.naver?mode=LSD&mid=shm&sid1=100\"\n",
    "\n",
    "# 사이트의 html 구조에 기반하여 크롤링을 수행합니다.\n",
    "driver = webdriver.Chrome(mac_path)  # for Mac\n",
    "# driver = webdriver.Chrome(window_path)  # for Windows\n",
    "driver.get(source_url)\n",
    "req = driver.page_source\n",
    "soup = BeautifulSoup(req, \"html.parser\")\n",
    "cluster_text = soup.find_all(name=\"div\", attrs={\"class\":\"cluster_text\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://n.news.naver.com/mnews/article/417/0000899014?sid=100\n",
      "https://n.news.naver.com/mnews/article/422/0000586143?sid=100\n",
      "https://n.news.naver.com/mnews/article/008/0004855984?sid=100\n",
      "https://n.news.naver.com/mnews/article/020/0003482210?sid=100\n",
      "https://n.news.naver.com/mnews/article/421/0006653606?sid=100\n"
     ]
    }
   ],
   "source": [
    "# a태그의 href 속성을 리스트로 추출하여, 크롤링 할 페이지 리스트를 생성합니다.\n",
    "page_urls = []\n",
    "for index in range(0, len(cluster_text)):\n",
    "    cluster = cluster_text[index]\n",
    "    news_url = cluster.find(name=\"a\", attrs={\"class\":\"cluster_text_headline nclicks(cls_pol.clsart)\"})\n",
    "    if news_url is not None:\n",
    "        page_urls.append(news_url.get(\"href\"))\n",
    "\n",
    "# 중복 url을 제거합니다.\n",
    "page_urls = list(set(page_urls))\n",
    "\n",
    "# 다섯 개의 페이지를 출력합니다.\n",
    "for page in page_urls[:5]:\n",
    "    print(page)\n",
    "\n",
    "# 크롤링에 사용한 브라우저를 종료합니다.\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![앱 생성 페이지](img/new_crawl_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 페이지내 텍스트 구조 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"28석만\"… 與, 이재명 체포동의안 관련 '이탈표' 촉구\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "국민의힘이 이재명 더불어민주당 대표에 대한 국회 체포동의안과 관련해 가결 쪽에 힘을 실으며 야당 측 이탈표를 호소하는 모양새다. 사진은 지난 24일 서울 여의도 국회에서 열린 제403회국회(임시회) 제7차 본회의에서 이 대표 체포동의안 및 기타 법안이 보고되는 모습. /사진=장동규 기자 국민의힘이 표결을 앞둔 이재명 더불어민주당 대표에 대한 체포동의안과 관련해 야당을 압박하며 막판 이탈표 호소에 나섰다. 국회는 이날 오후 2시30분 본회의를 열고 국회법에 따라 이 대표에 대한 체포동의안 표결에 들어간다. 국회는 지난 20일 특정경제범죄가중처벌등에관한법률(배임) 위반 혐의 등을 받는 이 대표에 대한 체포동의안을 접수했다.국회는 체포동의안을 접수하면 국회의장이 본회의에 보고하고 24시간 이후 72시간 이내에 무기명 투표를 진행한다. 이에 이 대표에 대한 체포동의안은 지난 24일 국회 본회의에서 보고된 뒤 이날 표결에 부쳐질 예정이다.체포동의안이 가결되려면 재적 의원 과반 출석에 과반 찬성이 필요하다. 국회 전체 의석 299석 중 민주당은 169석, 국민의힘 115석, 정의당 6석, 기본소득당 1석, 시대전환 1석, 무소속 7석 등이다. 민주당이 다수 의석을 가진 만큼 이 대표의 체포동의안 부결 가능성이 높게 점쳐지지만 민주당 안에서 28석 이상의 이탈표가 발생할 경우 체포동의안이 가결될 수 있다.이에 국민의힘은 비상대책위원회의와 원내대책회의·의원총회 등에서 체포동의안 가결과 이 대표 구속을 주장하며 '이재명·민주당 때리기'에 집중했다. 국회 의석 과반 이상인 민주당의 단합에 부결 가능성을 높게 점치고 있으면서도 결과와 관계없이 '방탄 국회' 이미지를 강조하며 여론의 우위를 선점한다는 계획이다.주호영 원내대표는 연일 이 대표의 체포동의안을 겨냥해 수위 높은 비판을 내뱉었다. 주 원내대표는 \"이 대표가 여러가지 부정부패 혐의를 받는 것은 민주당 뿐 아니라 국회 전체의 위신을 떨어뜨리고 있다\"며 \"불체포특권 포기를 공약했던 민주당, 특히 이 대표가 불체포특권 포기 공약을 지킬지 국민들은 지켜보고 있다\"고 압박했다. 표결 전망에 대해서는 \"양심 있는 민주당 의원이 많이 있을 것\"이라며 \"이번에 똘똘 뭉쳐서 체포동의안을 거부해도 그 후 방법이 없기에 그런 점을 많이 고민할 것\"이라고 예상했다.정진석 국민의힘 비상대책위원장 역시 지난 24일 \"이 대표가 민주당이 정말 현명한 생각을 할 때가 됐다\"며 \"지혜롭게 처신해주길 바란다\"고 불체포특권 포기를 촉구했다. 정 위원장은 이날에도 자신의 페이스북에 \"1987년 체제를 탄생시킨 민주화 운동권 세력이 집단 망상에 사로잡혀 기괴한 선택을 향해 달려가고 있다\"며 \"오늘 체포동의안이 부결된다면 우리는 한 세대 이상 이어져 온 1987년 체제의 종말·386 운동권 세대의 몰락을 지켜보게 될 것\"이라고 날을 세웠다.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome(mac_path)  # for Mac\n",
    "# driver = webdriver.Chrome(window_path)  # for Windows\n",
    "driver.get(page_urls[0])\n",
    "req = driver.page_source\n",
    "soup = BeautifulSoup(req, 'html.parser')\n",
    "title_area = soup.find(name=\"div\", attrs={\"class\":\"media_end_head_title\"})\n",
    "title = title_area.find_all('h2')[0]\n",
    "content_paragraphs = soup.find(name=\"div\", attrs={\"class\":\"newsct_article\"})\n",
    "content_corpus = content_paragraphs.text\n",
    "\n",
    "print(title.text)\n",
    "print(\"\\n\")\n",
    "print(content_corpus)\n",
    "\n",
    "# 크롤링에 사용한 브라우저를 종료합니다.\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [네이버 뉴스 헤드라인 데이터 크롤링]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 크롤링한 데이터를 데이터 프레임으로 만들기 위해 준비합니다.\n",
    "columns = [\"title\", \"content_text\"]\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# 각 페이지별 '제목', '본문' 정보를 데이터 프레임으로 만듭니다.\n",
    "for page_url in page_urls:\n",
    "\n",
    "    # 사이트의 html 구조에 기반하여 크롤링을 수행합니다.\n",
    "    driver = webdriver.Chrome(mac_path)  # for Mac\n",
    "    # driver = webdriver.Chrome(window_path)  # for Windows\n",
    "    driver.get(page_url)\n",
    "    req = driver.page_source\n",
    "    soup = BeautifulSoup(req, \"html.parser\")\n",
    "    \n",
    "    title_area = soup.find(name=\"div\", attrs={\"class\":\"media_end_head_title\"})\n",
    "    title = title_area.find_all('h2')[0]\n",
    "    content_paragraphs = soup.find(name=\"div\", attrs={\"class\":\"newsct_article\"})\n",
    "    content_corpus = content_paragraphs.text\n",
    "    \n",
    "    # 제목 정보에서 개행 문자를 제거한 뒤 추출합니다. 만약 없는 경우, 빈 문자열로 대체합니다.\n",
    "    if title is not None:\n",
    "        row_title = title.text.replace(\"\\n\", \" \")\n",
    "    else:\n",
    "        row_title = \"\"\n",
    "    \n",
    "    \n",
    "    # 모든 정보를 하나의 데이터 프레임에 저장합니다.\n",
    "    row = [row_title, content_corpus]\n",
    "    series = pd.Series(row, index=df.columns)\n",
    "    df = df.append(series, ignore_index=True)\n",
    "    \n",
    "    # 크롤링에 사용한 브라우저를 종료합니다.\n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"28석만\"… 與, 이재명 체포동의안 관련 '이탈표' 촉구</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n국민의힘이 이재명 더불어민주당 대표에 대한 국회 체포동의안과 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[속보] 윤대통령 \"학폭 근절 대책 조속히 보고\" 지시</td>\n",
       "      <td>\\n\\n\\t\\t\\t#윤대통령 #수석비서관회의 #교육부 #학교폭력 #근절대책연합뉴스T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>與 최고위원 후보 8인, 첫 토론회…\"총선승리 앞장서겠다\"</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n(서울=뉴스1) 이재명 기자 = 국민의힘 최고위원 후보들이 2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              title  \\\n",
       "0  \"28석만\"… 與, 이재명 체포동의안 관련 '이탈표' 촉구   \n",
       "1    [속보] 윤대통령 \"학폭 근절 대책 조속히 보고\" 지시   \n",
       "2  與 최고위원 후보 8인, 첫 토론회…\"총선승리 앞장서겠다\"   \n",
       "\n",
       "                                        content_text  \n",
       "0  \\n\\n\\n\\n\\n\\n국민의힘이 이재명 더불어민주당 대표에 대한 국회 체포동의안과 ...  \n",
       "1  \\n\\n\\t\\t\\t#윤대통령 #수석비서관회의 #교육부 #학교폭력 #근절대책연합뉴스T...  \n",
       "2  \\n\\n\\n\\n\\n\\n(서울=뉴스1) 이재명 기자 = 국민의힘 최고위원 후보들이 2...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 프레임을 출력합니다.\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 크롤링 이후의 분석 내용은 책에 기재되어 있는 내용인 `01-namu-wiki-text-analysis.ipynb` 파일과 동일합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
