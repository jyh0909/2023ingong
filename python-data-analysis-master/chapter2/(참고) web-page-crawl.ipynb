{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2019 [윤기태]\n",
    "\n",
    "https://github.com/yoonkt200/python-data-analysis\n",
    "\n",
    "[MIT License](https://github.com/yoonkt200/python-data-analysis/blob/master/LICENSE.txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (가제) 파이썬 데이터 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 웹 페이지 크롤링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 대상 페이지의 구조 살펴보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Internet Explorer, Chrome 등의 웹 브라우저로 크롤링의 대상이 될 페이지 구조 살펴보기\n",
    "    - 웹 브라우저의 개발자 도구 실행 (Chrome 기준 설명)\n",
    "        - `Chrome 맞춤설정 및 제어 탭 - 도구 더보기 - 개발자 도구` 클릭\n",
    "        - 혹은 `Command + Alt + I (Mac), Ctrl + Shift + I (윈도우)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![앱 생성 페이지](img/crawl_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 페이지들의 URL 정보 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 개발자 도구를 통해, 텍스트 데이터를 가져올 페이지들의 URL 리스트 추출\n",
    "    - HTML 구조에서 링크 정보 (**a 태그**) 추출\n",
    "        - 개발자도구 좌측 상단의 `마우스 모양` 버튼 클릭\n",
    "        - HTML 구조를 보고싶은 영역으로 마우스 커서 이동"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![앱 생성 페이지](img/crawl_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 아래 코드 실행을 위해, anaconda prompt 혹은 Terminal에서 아래와 같은 패키지들을 설치해 줍니다.\n",
    "    - (env_name) `pip install selenium beautifulsoup4`\n",
    "- 혹은 아래의 코드로 라이브러리를 설치합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.8.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /opt/homebrew/anaconda3/envs/test/lib/python3.9/site-packages (4.11.1)\n",
      "Collecting trio-websocket~=0.9\n",
      "  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
      "Collecting trio~=0.17\n",
      "  Downloading trio-0.22.0-py3-none-any.whl (384 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m384.9/384.9 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: urllib3[socks]~=1.26 in /opt/homebrew/anaconda3/envs/test/lib/python3.9/site-packages (from selenium) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /opt/homebrew/anaconda3/envs/test/lib/python3.9/site-packages (from selenium) (2022.12.7)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/homebrew/anaconda3/envs/test/lib/python3.9/site-packages (from beautifulsoup4) (2.3.2.post1)\n",
      "Collecting exceptiongroup>=1.0.0rc9\n",
      "  Downloading exceptiongroup-1.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting async-generator>=1.9\n",
      "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
      "Collecting outcome\n",
      "  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: idna in /opt/homebrew/anaconda3/envs/test/lib/python3.9/site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: sniffio in /opt/homebrew/anaconda3/envs/test/lib/python3.9/site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Collecting sortedcontainers\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/homebrew/anaconda3/envs/test/lib/python3.9/site-packages (from trio~=0.17->selenium) (22.2.0)\n",
      "Collecting wsproto>=0.14\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Collecting PySocks!=1.5.7,<2.0,>=1.5.6\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Collecting h11<1,>=0.9.0\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sortedcontainers, PySocks, outcome, h11, exceptiongroup, async-generator, wsproto, trio, trio-websocket, selenium\n",
      "Successfully installed PySocks-1.7.1 async-generator-1.10 exceptiongroup-1.1.0 h11-0.14.0 outcome-1.2.0 selenium-4.8.0 sortedcontainers-2.4.0 trio-0.22.0 trio-websocket-0.9.2 wsproto-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ww/prhhsq7100g92nt7xr4ltql5lkv223/T/ipykernel_30754/3517989882.py:17: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(mac_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://namu.wiki/w/%ED%99%8B%EC%BD%94%20%ED%83%80%EB%A3%A8%EB%A7%88%EC%97%90(%EC%9A%B0%EB%A7%88%EB%AC%B4%EC%8A%A4%EB%A9%94%20%ED%94%84%EB%A6%AC%ED%8B%B0%20%EB%8D%94%EB%B9%84)\n",
      "https://namu.wiki/w/%EB%A1%9C%EB%B2%84%ED%8A%B8%20%EB%93%80%EB%B0%9C\n",
      "https://namu.wiki/w/%EB%82%A8%EC%9E%90%EA%B0%80%20%EC%82%AC%EB%9E%91%ED%95%A0%20%EB%95%8C(SBS%20%EB%93%9C%EB%9D%BC%EB%A7%88)\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# brew 로 설치된 chromedriver의 path (Mac)\n",
    "# 실행 권한 오류가 생길 시, chromedriver가 있는 폴더로 이동하여 `xattr -d com.apple.quarantine chromedriver` 명령어 입력\n",
    "mac_path = \"/usr/local/bin/chromedriver\"  # m1 이전 맥 OS\n",
    "# mac_path = \"/opt/homebrew/bin/chromedriver\"  # m1 이후 맥 OS\n",
    "\n",
    "# 윈도우용 크롬 웹드라이버 실행 경로 (Windows)\n",
    "window_path = \"chromedriver.exe\"\n",
    "\n",
    "# 크롤링할 사이트 주소를 정의합니다.\n",
    "source_url = \"https://namu.wiki/RecentChanges\"\n",
    "\n",
    "# 크롬 드라이버를 사용합니다 (맥은 첫 줄, 윈도우는 두번째 줄 실행)\n",
    "driver = webdriver.Chrome(mac_path)\n",
    "\n",
    "# 드라이버가 브라우징 할 페이지 소스를 입력합니다\n",
    "driver.get(source_url)\n",
    "req = driver.page_source\n",
    "\n",
    "# 사이트의 html 구조에 기반하여 데이터를 파싱합니다.\n",
    "soup=BeautifulSoup(req, \"html.parser\")\n",
    "contents_table = soup.find(name=\"table\")\n",
    "table_body = contents_table.find(name=\"tbody\")\n",
    "table_rows = table_body.find_all(name=\"tr\")\n",
    "\n",
    "# a태그의 href 속성을 리스트로 추출하여, 크롤링 할 페이지 리스트를 생성합니다.\n",
    "page_url_base = \"https://namu.wiki\"\n",
    "page_urls = []\n",
    "for index in range(0, len(table_rows)):\n",
    "    first_td = table_rows[index].find_all(\"td\")[0]\n",
    "    td_url = first_td.find_all(\"a\")\n",
    "    if len(td_url) > 0:\n",
    "        page_url = page_url_base + td_url[0].get(\"href\")\n",
    "        if \"png\" not in page_url:\n",
    "            page_urls.append(page_url)\n",
    "\n",
    "# 중복 url을 제거합니다.\n",
    "page_urls = list(set(page_urls))\n",
    "for page in page_urls[:3]:\n",
    "    print(page)\n",
    "    \n",
    "# 크롤링에 사용한 브라우저를 종료합니다.\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위의 코드를 통해 https://namu.wiki/RecentChanges 페이지의 최근 변경 문서 링크들을 추출."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 각종 텍스트 정보 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 동일한 방법으로, 개발자 도구를 이용하여 최근 변경된 나무위키 페이지의 텍스트 정보 추출\n",
    "    - `문서 제목`\n",
    "    - `문서 카테고리`\n",
    "    - `문서 내용`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![앱 생성 페이지](img/crawl_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![앱 생성 페이지](img/crawl_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![앱 생성 페이지](img/crawl_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `2. 페이지들의 URL 정보 추출` 에서 추출했던 URL 페이지 정보를 기반으로 크롤링 실행."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하나의 최근 변경된 문서를 크롤링합니다.\n",
    "\n",
    "# 크롬 드라이버를 사용합니다 (맥은 첫 줄, 윈도우는 두번째 줄 실행)\n",
    "driver = webdriver.Chrome(mac_path)\n",
    "# driver = webdriver.Chrome(window_path)\n",
    "\n",
    "# 드라이버가 브라우징 할 페이지 소스를 입력합니다\n",
    "driver.get(page_urls[0])\n",
    "req = driver.page_source\n",
    "soup=BeautifulSoup(req, \"html.parser\")\n",
    "contents_table = soup.find(name=\"article\")\n",
    "title = contents_table.find_all(\"h1\")[0]\n",
    "category = contents_table.find_all(\"ul\")[0]\n",
    "content_paragraphs = contents_table.find_all(name=\"div\", attrs={\"class\":\"wiki-paragraph\"})\n",
    "content_corpus_list = []\n",
    "\n",
    "for paragraphs in content_paragraphs:\n",
    "    content_corpus_list.append(paragraphs.text)\n",
    "content_corpus = \"\".join(content_corpus_list)\n",
    "\n",
    "print(title.text)\n",
    "print(\"\\n\")\n",
    "print(category.text)\n",
    "print(\"\\n\")\n",
    "print(content_corpus)\n",
    "\n",
    "# 크롤링에 사용한 브라우저를 종료합니다.\n",
    "driver.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
